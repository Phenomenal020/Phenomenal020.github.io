define({ entries : {
    "10268408": {
        "abstract": "Affective computing develops systems, which recognize or influence aspects of human life related to emotion, including feelings and attitudes. Significant potential for both good and harm makes it ethically sensitive, and trying to strike sound balances is challenging. Common images of the issues invite oversimplification and offer a limited understanding of the moral consequences and ethical tensions. Considering the state-of-the-art shows how pervasive and complex they are. In many areas, the discipline can potentially bring ethically significant benefits and hence has a duty to try. They include making interactions with machines more effective and less stressful, diagnostic and therapeutic roles in emotion-related disorders, intelligent tutoring, and reducing isolation. However, the limits of recognition technology mean that actions are likely to be based on impoverished representations of people\u2019s affective state, particularly with certain groups; systems are liable to arouse feelings that are positive, but not well grounded in reality, affectively engaging systems can become addictive and manipulative, and they confer dangerous power on those who control the technology. We offer an overview of those and other particular ethical issues, positive and negative, which arise from the current state of affective computing. It aims to reflect the complexities inherent in both the technology and current ethical discussions. Establishing appropriate responses is a challenge for society as a whole, not only the affective computing community.}.",
        "author": "Devillers, Laurence and Cowie, Roddy",
        "doi": "10.1109/JPROC.2023.3315217",
        "journal": "Proceedings of the IEEE",
        "keywords": "Ethics;Affective computing;Complexity theory;Standards;Emotion recognition;Sentiment analysis;Human factors;Affective computing;affective computing applications;ethical/societal implications;influencing human emotional state",
        "number": "10",
        "pages": "1445-1458",
        "title": "Ethical Considerations on Affective Computing: An Overview",
        "type": "ARTICLE",
        "volume": "111",
        "year": "2023"
    },
    "10388144": {
        "abstract": "There has been a rapid transformation in the medium of learning and communication due to the pandemic. Multitudes have adopted online video platforms to learn and work from any corner of the world. Emotion detection is vital for understanding how well instructions are communicated through online interactions and for building cognitive systems that can identify human behavior. Confusion is a key emotion that can impact online learning and can be used to verify whether students using an online platform understand the material being taught. Our research expands on previous work regarding confusion detection, focusing on data fusion techniques. We explore the impact of early fusion (feature-level) vs late fusion (decision-level) on modeling confusion identification during a collaborative block building task. Experimenting with different classifiers, our results show that late fusion performs better with larger time windows. This fusion approach can aid in model interpretability.}.",
        "author": "Ashwath, Anisha and Peechatt, Michael and Alm, Cecilia and Bailey, Reynold",
        "booktitle": "2023 11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)",
        "doi": "10.1109/ACIIW59127.2023.10388144",
        "keywords": "Solid modeling;Emotion recognition;Pandemics;Buildings;Neural networks;Data integration;Collaboration;data fusion;multimodal data;affective computing;early fusion;late fusion",
        "pages": "1-4",
        "title": "Early vs. Late Multimodal Fusion for Recognizing Confusion in Collaborative Tasks",
        "type": "INPROCEEDINGS",
        "year": "2023"
    },
    "5543262": {
        "abstract": "In 2000, the Cohn-Kanade (CK) database was released for the purpose of promoting research into automatically detecting individual facial expressions. Since then, the CK database has become one of the most widely used test-beds for algorithm development and evaluation. During this period, three limitations have become apparent: 1) While AU codes are well validated, emotion labels are not, as they refer to what was requested rather than what was actually performed, 2) The lack of a common performance metric against which to evaluate new algorithms, and 3) Standard protocols for common databases have not emerged. As a consequence, the CK database has been used for both AU and emotion detection (even though labels for the latter have not been validated), comparison with benchmark algorithms is missing, and use of random subsets of the original database makes meta-analyses difficult. To address these and other concerns, we present the Extended Cohn-Kanade (CK+) database. The number of sequences is increased by 22\\% and the number of subjects by 27\\%. The target expression for each sequence is fully FACS coded and emotion labels have been revised and validated. In addition to this, non-posed sequences for several types of smiles and their associated metadata have been added. We present baseline results using Active Appearance Models (AAMs) and a linear support vector machine (SVM) classifier using a leave-one-out subject cross-validation for both AU and emotion detection for the posed data. The emotion and AU labels, along with the extended image data and tracked landmarks will be made available July 2010.}.",
        "author": "Lucey, Patrick and Cohn, Jeffrey F. and Kanade, Takeo and Saragih, Jason and Ambadar, Zara and Matthews, Iain",
        "booktitle": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops",
        "doi": "10.1109/CVPRW.2010.5543262",
        "keywords": "Databases;Gold;Active appearance model;Support vector machines;Support vector machine classification;Face detection;Testing;Performance evaluation;Measurement;Code standards",
        "pages": "94-101",
        "title": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression",
        "type": "INPROCEEDINGS",
        "year": "2010"
    },
    "8450969": {
        "abstract": "The field of education has been affected by globalization and the constant increase of online courses. The high number of students enrolled in these learning environments and their constant interaction with platforms generate a large amount of data that is difficult to handle with traditional methods of data analysis. The permanence of students in these courses poses challenges aimed at raising their level of commitment and motivation. Several articles with this approach have been identified in the literature analyzed in this work. Some of them are related to the application of text mining techniques aimed at analyzing the interaction of students in these environments. This interaction is based on entries included in discussion forums, emails or interaction in social networks. In this article, we explore the interaction of students through text mining techniques in different student interaction environments in a massive open online course (MOOC). The research focuses on the calculation and analysis of the frequency of terms, the analysis of concordances and groupings in n-grams.}.",
        "author": "Buena\u00f1o-Fern\u00e1ndez, Diego and Villegas-Ch, W. and Luj\u00e1n-Mora, Sergio",
        "booktitle": "2018 IEEE World Engineering Education Conference (EDUNINE)",
        "doi": "10.1109/EDUNINE.2018.8450969",
        "keywords": "Electronic mail;Text mining;Education;Twitter;Tools;Text mining;Massive Open Online Course;MOOC;Supervised learning;Opinion mining",
        "pages": "1-6",
        "title": "Using text mining to evaluate student interaction in virtual learning environments",
        "type": "INPROCEEDINGS",
        "year": "2018"
    },
    "9395844": {
        "abstract": "This study aims to recognize the deep features in speech for the emotion recognition task, with less complex architecture and fewer learnable parameters. We have proposed a simple CNN (convolutional neural network) architecture, based on log-mel-spectrograms of segmented speech utterances. The proposed architecture is used to extract the emotion-related features for two principally used databases in speech emotion recognition applications, Interactive emotional dyadic motion capture (IEMOCAP) and the Berlin database of emotional speech (EmoDB) databases. Several extensive experiments on these datasets demonstrate the performance of the proposed model and the results are compared with the recent CNN architectures. For speaker-independent analysis, the proposed CNN network achieves classification accuracies of 59.33\\% and 65.47\\% on IEMOCAP and improvised IEMOCAP utterances respectively for four emotional classes, and 72.02\\% for the Berlin EmoDB databases for seven classes.}.",
        "author": "Chauhan, Krishna and Sharma, Kamalesh Kumar and Varma, Tarun",
        "booktitle": "2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS)",
        "doi": "10.1109/ICAIS50930.2021.9395844",
        "keywords": "Emotion recognition;Databases;Motion segmentation;Neural networks;Speech recognition;Feature extraction;Task analysis;CNN;Speech emotion recognition;log-mel-spectrogram",
        "pages": "1176-1181",
        "title": "Speech Emotion Recognition Using Convolution Neural Networks",
        "type": "INPROCEEDINGS",
        "year": "2021"
    },
    "9815154": {
        "abstract": "In this article, behaviour of students in the e-learning environment is analyzed. The novel pipeline is proposed based on video facial processing. At first, face detection, tracking and clustering techniques are applied to extract the sequences of faces of each student. Next, a single efficient neural network is used to extract emotional features in each frame. This network is pre-trained on face identification and fine-tuned for facial expression recognition on static images from AffectNet using a specially developed robust optimization technique. It is shown that the resulting facial features can be used for fast simultaneous prediction of students\u2019 engagement levels (from disengaged to highly engaged), individual emotions (happy, sad, etc.,) and group-level affect (positive, neutral or negative). This model can be used for real-time video processing even on a mobile device of each student without the need for sending their facial video to the remote server or teacher's PC. In addition, the possibility to prepare a summary of a lesson is demonstrated by saving short clips of different emotions and engagement of all students. The experimental study on the datasets from EmotiW (Emotion Recognition in the Wild) challenges showed that the proposed network significantly outperforms existing single models.}.",
        "author": "Savchenko, Andrey V. and Savchenko, Lyudmila V. and Makarov, Ilya",
        "doi": "10.1109/TAFFC.2022.3188390",
        "journal": "IEEE Transactions on Affective Computing",
        "keywords": "Feature extraction;Face recognition;Electronic learning;Emotion recognition;Task analysis;Training;Convolutional neural networks;Online learning;e-learning;video-based facial expression recognition;engagement prediction;group-level emotion recognition;mobile devices",
        "number": "4",
        "pages": "2132-2143",
        "title": "Classifying Emotions and Engagement in Online Learning Based on a Single Facial Expression Recognition Neural Network",
        "type": "ARTICLE",
        "volume": "13",
        "year": "2022"
    },
    "9826770": {
        "abstract": "Due to the COVID-19 pandemic, most universities have adapted their learning infrastructure to an increasing demand for online training modalities. However, this type of learning, usually through Learning Management Systems (LMSs), suffer from a lack of direct feedback between students and the educational staff. For that reason, the present work introduces the EMO-learning project, whose key goal is to capture the emotions of students. This is done by means of a deep learning approach, able to timely analyse the face expressions of the students during online lectures. The module has been tested with different students during the academic year 2020-21, showing quite promising results.}.",
        "author": "L\u00f3pez, Bel\u00e9n and Arcas-T\u00fanez, Francisco and Cantabella, Magdalena and Terroso-S\u00e1enz, Fernando and Curado, Manuel and Mu\u00f1oz, Andr\u00e9s",
        "booktitle": "2022 18th International Conference on Intelligent Environments (IE)",
        "doi": "10.1109/IE54923.2022.9826770",
        "keywords": "Training;Deep learning;COVID-19;Learning management systems;Emotion recognition;Pandemics;Face recognition;smart education;emotion recognition;affective computing;neural networks;COVID-19",
        "pages": "1-4",
        "title": "EMO-Learning: Towards an intelligent tutoring system to assess online students\u2019 emotions",
        "type": "INPROCEEDINGS",
        "year": "2022"
    },
    "Busso2008IEMOCAP": {
        "abstract": "Since emotions are expressed through a combination of verbal and non-verbal channels, a joint analysis of speech and gestures is required to understand expressive human communication. To facilitate such investigations, this paper describes a new corpus named the \u201cinteractive emotional dyadic motion capture database\u201d (IEMOCAP), collected by the Speech Analysis and Interpretation Laboratory (SAIL) at the University of Southern California (USC). This database was recorded from ten actors in dyadic sessions with markers on the face, head, and hands, which provide detailed information about their facial expressions and hand movements during scripted and spontaneous spoken communication scenarios. The actors performed selected emotional scripts and also improvised hypothetical scenarios designed to elicit specific types of emotions (happiness, anger, sadness, frustration and neutral state). The corpus contains approximately 12 h of data. The detailed motion capture information, the interactive setting to elicit authentic emotions, and the size of the database make this corpus a valuable addition to the existing databases in the community for the study and modeling of multimodal and expressive human communication.}.",
        "author": "C. Busso and M. Bulut and C. C. Lee and E. Kazemzadeh and A. M. Carreras and M. Cristen and I. Cohen and S. Narayanan",
        "doi": "10.1007/s10579-008-9076-6",
        "journal": "Lang Resources \\& Evaluation",
        "pages": "335--359",
        "title": "IEMOCAP: Interactive Emotional Dyadic Motion Capture Database",
        "type": "article",
        "url": "https://doi.org/10.1007/s10579-008-9076-6",
        "volume": "42",
        "year": "2008"
    },
    "chen2018emotionlines": {
        "abstract": "Feeling emotion is a critical characteristic to distinguish people from machines. Among all the multi-modal resources for emotion detection, textual datasets are those containing the least additional information in addition to semantics, and hence are adopted widely for testing the developed systems. However, most of the textual emotional datasets consist of emotion labels of only individual words, sentences or documents, which makes it challenging to discuss the contextual flow of emotions. In this paper, we introduce EmotionLines, the first dataset with emotions labeling on all utterances in each dialogue only based on their textual content. Dialogues in EmotionLines are collected from Friends TV scripts and private Facebook messenger dialogues. Then one of seven emotions, six Ekman\u2019s basic emotions plus the neutral emotion, is labeled on each utterance by 5 Amazon MTurkers. A total of 29,245 utterances from 2,000 dialogues are labeled in EmotionLines. We also provide several strong baselines for emotion detection models on EmotionLines in this paper",
        "archiveprefix": "arXiv",
        "author": "S.-Y. Chen, C.-C. Hsu, C.-C. Kuo, T.-H. Huang, and L.-W. Ku",
        "doi": "https://doi.org/10.48550/arXiv.1802.08379",
        "eprint": "1802.08379",
        "primaryclass": "cs.CL",
        "title": "EmotionLines: An Emotion Corpus of Multi-Party Conversations",
        "type": "misc",
        "year": "2018"
    },
    "inproceedings": {
        "abstract": "The article describes a database of emotional speech. Ten actors (5 female and 5 male) simulated the emotions, producing 10 German utterances (5 short and 5 longer sentences) which could be used in everyday communication and are interpretable in all applied emotions. The recordings were taken in an anechoic chamber with high-quality recording equipment. In addition to the sound electro-glottograms were recorded. The speech material comprises about 800 sentences (seven emotions * ten actors * ten sentences + some second versions). The complete database was evaluated in a perception test regarding the recognisability of emotions and their naturalness. Utterances recognised better than 80\\% and judged as natural by more than 60\\% of the listeners were phonetically labelled in a narrow transcription with special markers for voice-quality, phonatory and articulatory settings and articulatory features. The database can be accessed by the public via the internet (http://www.expressive-speech.net/emodb/).",
        "author": "Burkhardt, Felix and Paeschke, Astrid and Rolfes, M. and Sendlmeier, Walter and Weiss, Benjamin",
        "doi": "10.21437/Interspeech.2005-446",
        "journal": "9th European Conference on Speech Communication and Technology",
        "month": "09",
        "number": "4",
        "pages": "1517-1520",
        "title": "A database of German emotional speech",
        "type": "inproceedings",
        "year": "2005"
    }
}});